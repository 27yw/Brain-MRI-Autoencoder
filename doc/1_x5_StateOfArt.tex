\chapter{State of art: related works}
\label{chapter:stateofart}

\section{Overview}
The world relevance and impact of this problem is also shown in the related articles of this subject. The state of art of Deep Learning applied to brain MRI shows the relevance of this field. As we discussed in the introduction, in section \ref{chapter:introduccion}, different deep learning techniques have been used to address the problems derived from brain MRI images: \textbf{classification healthy/disease, tumor segmentation, optimize data acquisition, data augmentation and image enhancement} are the principal ones. 

\begin{tcolorbox}
Nevertheless, we must highlight that all of this problems have common points of works. One of them is the purpose of our project: \textbf{learn MRI representation for reconstruction}.
\end{tcolorbox}

The advance in some of the questions leads to the advance in another. \textbf{Image reconstruction}, which is a mainly sub-problem of image enhancement, could help to achieve better results in:
\begin{itemize}
    \item \textbf{Data acquisition}
    \begin{itemize}
        \item Reconstruct the image from less data collected: faster scanning process \cite{fastmri}.
    \end{itemize}
    
    \item \textbf{Disease detection and segmentation}
    \begin{itemize}
        \item Unsupervised Anomaly Detection: detect diseases with non-labeled data: reconstruction of the disease image differs more than the pathology-free one \cite{pinaya2019}.
        \item Tumor segmentation (widely known as BraTS \cite{brats2014}): encode for extract deep image features and decoder for reconstruction of dense segmentation mask \cite{myronenko20183d}.
    \end{itemize} 
    
    \item \textbf{Data Augmentation}
    \begin{itemize}
        \item Construction of pathology-free image from abnormal and viceversa \cite{2020inpainting} (i.e. lesion inpainting).
        \item Artificial MRI Generation \cite{GanDataAugment2018} \myurl{https://paperswithcode.com/paper/generation-of-3d-brain-mri-using-auto}{\cite{kwon2019gangeneration}} .
    \end{itemize}
    
    \item \textbf{Image Enhancement}
    \begin{itemize}
        \item Reconstruction of cropped parts.
        \item Reconstruction without noise and artifacts \cite{superresolution} \cite{bermudez2018t1autoencoder}, \cite{gondara2016medicalautoencoder}, \cite{wganautoencoder}.
        \item Definition enhancement: from low resolution to high resolution \cite{ganHR3d} \cite{superresolution}.
    \end{itemize}
\end{itemize} 

We want to emphasize the actual relevance of this project. Solving MRI problems using Deep Learning isn't just about how to apply Deep Learning to another field. It is not just a Deep Learning experiment to demonstrate the power of this method. Solving problems with MRI diagnostics (classification, segmentation), MRI quality (MRI enhancement, data augmentation), or MRI acquisition are cutting edge issues in both the field of Computer Science and Healthcare (neuroimaging, neurological analysis, etc.). 



\section{Related works}

We realize this in the overwhelming number of articles using different Deep Learning architectures for solving all kinds of problems with MRI. We will discuss papers addressing different problems but with one similarity: use of image reconstruction in some part of the process (preferably by using autoencoder-based solution). \textbf{However, the main purpose for this project is to apply this reconstruction techniques for noise reduction (image enhancement) and data augmentation (lesion inpainting).}

The main evidence of the big relevance and collaboration between Deep Learning and MR imaging is \textbf{FastMRI by Facebook AI}. In fact, lately, the focus is on \textbf{improving MRI acquisition}, with techniques based on collecting fewer data and using \underline{reconstruction} techniques with Deep Learning with the aim of improving image quality and acquisition speed. The high-impact in the academic field of these kind of studies is based on Facebook AI works. Facebook AI is focused on \textbf{accelerating MR imaging} with AI, and it is his main goal in healthcare nowadays. They created \myurl{https://fastmri.org/}{fastMRI} \cite{fastmri}, a set of models working with some benchmark datasets in order to accelerate the MR imaging acquisition. It is open source, and you can participate in the \myurl{https://fastmri.org/submission_guidelines/}{challenge} with data from New York University. Recently, Facebook and \myurl{https://sites.google.com/view/med-neurips-2020}{NeurIPS} announced that the best models and projects presented for this purpose, even from groups outside Facebook, will be invited to NeurIPS, one of the most important conferences on Neural Information Processing Systems.

%% NVIDIA tumor segmentation
To continue with the different studies using reconstruction methods for distinct purposes, we describe the use of reconstruction for helping \textbf{Tumor Segmentation}. This is another main problem in the state of the art. There is a global academic challenge using labeled brain tumor MRI for BRAin Tumor Segmentation called \textbf{BRATS} \cite{brats2014}. This competition is compound by a MRI dataset from T1, T1c, T2 and FLAIR MRI and the goal is make the segmentation of the distinct parts of the tumor. Using this data as a \myurl{https://paperswithcode.com/task/brain-tumor-segmentation}{benchmark}, lots of different groups are making experiments each year to improve the results. One of these studies using \underline{reconstruction} techniques is the current best outcome for BRAST 2018: \myurl{https://paperswithcode.com/paper/3d-mri-brain-tumor-segmentation-using}{\textbf{A. Myronenko} \cite{myronenko20183d}}. Although their objective is the 3D segmentation of tumors, they use a curious architecture, shown in figure \ref{fig:figs/architecture_myronenko.PNG}, that incorporates an encoder and two decoding branches: one for the creation of tumor segmentation masks and the other for the reconstruction of images. This branch of image reconstruction is only used during training as an additional guide to regularize  the encoder part. The encoder is made by \textbf{ResNet} blocks (Group Norm+ReLu+Conv). The decoder is a \textbf{variational autoencoder} (VAE) made of the distribution layer and deconvolutional upsampling layers with Group Normalization and ReLu.
2 more parts are incorporated in their main loss function for tumor segmentation: Mean square error and Kullback–Leibler divergence of the reconstruction branch.

\imagen{figs/architecture_myronenko.PNG}{Architecture of ResNet-VAE-based network of A. Myronenko. \cite{myronenko20183d}}

%% Classification and pinaya - Simple convolutional autoencoder -semisuepr
Another problem is to \textbf{classify whether an image belongs to a control or a patient}. A recent approach is based on the construction of normative models \cite{marquand2016normative}, and , therefore, the image \underline{reconstruction} based in this normative model. \textbf{Pinaya et al}. \cite{pinaya2019} use this technique to identify abnormal patterns in neuropshychiatric disorders towards achieving \textbf{unsupervised anomaly detection}, so we don't need labeled images from disease data.
Classic methods and approaches based on \textbf{sMRI} (structural magnetic resonance imaging) can't get a good performance in abnormal brain structural detection because neuroanatomical alterations in neurological disorders can be subtle and spatially distributed. Another approach based on Machine Learning methods could improve performance because algorithms are sensitive to these subtle characteristics. The downside of this road is the need for a large amount of image data (control and disease) and that the models are black-boxes with no information on the critical characteristics used for the decision. They developed a \textbf{Deep semi-supervised Autoencoder}, which put unsupervised anomaly detection up for discussion.
The goal of that study is to build an autoencoder which encode the structure of control brains. It means the autoencoder learn the normal distribution for healthy brains and the abnormal MR images would be outliers in that distribution. With this autoencoder defining a distribution for control patients, they define a \textbf{deviation metric} to measure the neuroanatomical deviation in patients. Patients with some disorder should be outliers in this distribution. The architecture and technique used in the experiment is the following:
\begin{itemize}
\item Architecture
    \begin{itemize}
        \item Semi-supervised autoencoder: reconstruction of the image and prediction of age and sex.
        \item 3 hidden layers with SELUs activation function.
        \item Output layer with Linear activation function.
        \item Loss function: MSE from reconstructed and original image + cross-entropy for age + cross-entropy for years + Unsupervised cross-covariance.
        \item 2000 epochs.
        \item ADAM optimizer (adaptive moment estimation) with adaptative learning rate.
        \item 64 samples mini-batches.
    \end{itemize}
\item Transformation of input data:
    \begin{itemize}
        \item Add Gaussian noise to image (0, 0.1).
        \item Feature scaling (normalization).
        \item One-hot encoding for \texttt{sex} and \texttt{age} labels.
    \end{itemize}
\end{itemize}

%% Contar inpaint lessons autoencoder
We continue with a special case: \textbf{lesion inpainting} \cite{2020inpainting}. It can be seen as a \textbf{data augmentation} task (\underline{reconstructing} ‘pathology-free’ versions from patients with any brain disease). In the work of \textbf{José V. Majón et. al.} \cite{2020inpainting} the medial purpose is \textbf{the improvement of the behavior of current brain image analysis pipelines}.  These pipelines are not robust to brain MR images with lesions. For example, a task such as brain part segmentation decreases its accuracy when dealing with lesions. They proposed a \textbf{3D UNet} like network to map the image with lesion to the inpainted image (target). The encoder is made by 3 blocks of a 3D Convolutional Layer with ReLU activation, Batch Normalization and max-pooling. For the decoder they used same architecture but upsampling instead of max-pooling and, in the last step, a  tri-linear interpolation layer followed by a 3D convolution layer (with 8 filters) plus a ReLU and Batch normalization layers for upsampling the image. We can see the diagram pf the architecture in figure \ref{fig:figs/architecture_manjon.PNG}. Everything was trained with MSE loss function. They use lesion masks to generate artificial training data. The use control cases masked out with lesion masks using the software lesionBrain \cite{lesionBrain}.

\imagen{figs/architecture_manjon.PNG}{U-Net based architecture of network of J. V. Manjon et. al. \cite{2020inpainting}}


Besides of all of these principal studies and objectives, there are so many more. \myurl{ https://github.com/TheoEst/
joint_registration_tumor_segmentation}{Théo Estienne et. al.} \cite{otherBraTS2020} in 2020 realize a project based in the study from A. Myronenko \cite{myronenko20183d} which we explained before. They also research Deep Learning architectures for tumor segmentation (BraTS 2018) and use a autoencoder-based network with 2 decoder branches: one for tumor segmentation an another for reconstruction. This last branch is only used for encoder-regularization. They use a fully convolutional VNet architecture, with convolutional layers, ReLU activation function and a intra-block residual
connection with the output of the first activated convolution of the corresponding block. They use also direct connections from encoder to decoder part. 

We discover another study from \myurl{https://paperswithcode.com/paper/a-convolutional-autoencoder-approach-to-learn}{Evan M. Yu et. al. \cite{learnvolrepreCODE}} in which they try to learn volumetric representations from different parts of brain structure. They also use an autoencoder framework.They architecture is composed by 2 components: a spatial transformer network (STN) and a convolutional autoencoder (CAE). The autoencoder is a kind of ResNet-based one, because it uses residual blocks with skip connections, instance normalization and Leaky ReLU activation function.

In order to finalize with the review of the studies of reconstruction applications in MRI, we want to explain one last paper. This work is not focused in MRI, but it achieves very good performance in many tasks like restoration, denoising, super-resolution or image inpainting. \myurl{https://paperswithcode.com/paper/image-restoration-using-convolutional-auto}{XJ. Mao et all \cite{superresolution}} published in 2016 an study about CAE with symetric skip-connections to address those objectives. They also use a residual based network which they call RED-Net. The main characteristics of this network are the skip connections (in which one layer from the encoder are added up to its symmetric layer in the decoder) and the lack of pooling layers. They don't use pooling layers due to pooling discards useful image details that are essential for these tasks. They use MSE loss function. Peak Signal-to-Noise Ratio
(PSNR) and Structural SIMilarity (SSIM) index are calculated for evaluation.

The studies and tasks just explained are the prominent and recent ones but there are other areas and analysis in which MRI restoration could help. Some of them are survival prediction \cite{AnexoReviewAditional}, disease progression \cite{AnexoProgression} or brain connectivity analysis   \cite{AnexoConnectivity}.

\section{Volumes or slices?}
\label{section:soa_vols_slices}

Brain MR images are stored in volumes. It means it are stored as 3D volumes representing somebody's head. Some projects directly use 3D volumes to achieve the purpose (i.e. 3D tumor segmentation or 3D reconstruction). It is more complicated get good results in 3D than in 2D, because results are more relevant in the field. 

When working in 2D we have to consider another decisions. The first one is \textbf{what profile of the volume should we use}. Brain MRI Volume has 3 different views: \textbf{axial} (from above the head), \textbf{sagittal} (from the side of the face, profile) and \textbf{coronal} (from behind the head). We can see the different views in figure \ref{fig:figs/mriviews.jpg}

\imagen{figs/mriviews.jpg}{Left: axial. Middle: sagittal. Right: coronal}

For non-isotropic acquisitions, we should ideally slice them so that the slices are high resolution. For example, if the \textbf{voxel} resolution is 1x1x5 $mm^3$, we should slice the volume so that the slices are 1x1$mm^2$rather than 1x5$mm^2$ (or 5x1$mm^2$). The other issue to address is \textbf{what 2D slices from the volume has relevant information}. Some of the slices are slices of the extremes of the volume, and it didn't represent relevant information about the brain structure-

In this project, we are going to work with 2D slices due to time constraints. Therefore, in this section, we will discuss whether the projects mentioned above have used volumes or images, from which volume profile they have taken the images and how the ones containing important information have been chosen. Our approach will be explained in section \ref{subsubsection:relevantsliceselection}.

A. Myronenko \cite{myronenko20183d} uses 3D volumes for the brain tumor segmentation task (BraTS 2018) with 1x1x1 mm isotropic resolution and size 240x240x155. Consequently, he does not have to get any profile or select slices. Pinaya et. al \cite{pinaya2019} use T1 weighted images, thus 2D slices. They don't say neither the profile used in the images, nor the method to select 2D slices with relevant information. To continue, Manjón et. al \cite{2020inpainting} propose "the first 3D blind inpainting method in medical imaging" as they say. They use the same dataset that us (IXI), they work directly with 3D volumes, so neither profile nor slice election is done. They only preprocessed the volumes in order to normalize the voxels in to 1 mm3 voxel resolution. More recently, Théo Estienne et.al \cite{otherBraTS2020} also address the problem of 3D tumor segmentation of BraTS 2018. Evan M. Yu et. al. \cite{learnvolrepreCODE} uses 3D volumes of OASIS dataset to learn volumetric shape representations for brains structures.

As we can conclude, most projects address 3D volumes because his relevant implications, both in medical field and in deep learning field. However, we have found another articles in which they work with 2D slices. \textbf{C. Bermudez, et al} \cite{bermudez2018t1autoencoder} uses 2D \textbf{axial} slices from BLSA dataset. All subjects were affine-registered to MNIs-space and intensity-normalized before 2D slices are selected. In addition, the only select a \textbf{single midline axial slice} from each volume. Finally they had 528 images with size 220 x 170 voxels. They use this 2D images to build 3 denoiser autoencoders with skip connections (one autoencoder for each level of noise added to train data). The architecture of this 2D denoiser brain mri autoencoder is shown in figure \ref{fig:figs/cbermudezarchitecture.jpg}.

\imagen{figs/cbermudezarchitecture.jpg}{C. Bermudez et. al. Denoising CAE+skip-connections architecture \cite{bermudez2018t1autoencoder}}

\FloatBarrier
\clearpage



\section{Network architectures for images}
%% Contar u-net, resnet VGG, DenseNet, variational

The encoder part of the autoencoder should work as a feature extractor, so we can research the most known Deep Learning Architectures for images in order to use the same architecture or realize transfer learning (with frozen weights or not). We will start explaining the first approach to image processing with Deep Learning, then we will continue with residual network architectures (commonly used in related works) and we will conclude by explaining U-Net and V-Net, the other 2 commonly used networks for medical image segmentation.

\subsection{Alexnet}

First of all, we are going to introduce \textbf{Alexnet} \cite{alexnet}, a deep convolutional neural network for image classification created by Alex Krizhevsky et. al in 2012 which, in that moment, was the best approach for ImageNet classification, improving by 10\% of difference with the second best. It is very important because they apply Deep Learning and convolutional networks to the classification of images and they establish a standard from which new networks are created by adding improvements. The architecture, shown in table \ref{table:alexnet}, is made by 5 convolutional layers and max pooling layer after convolutional 1, 2 and 5 layers, and 3 dense (fully connected) layers as classifier part. Rectified linear unit is used a a activation function (ReLU). Softmax function is used in the last layer in order to represent a probability distribution over the image classes. So AlexNet is characterized by \textbf{convolutional layers, max pooling layers, dense layers as classifier and ReLU activation function}.

Since the moment Alexnet appeared, the improvements made were almost everyone about go deeper in the layers and architecture. However, going deeper didn't solve another problems like vanishing gradient.

\begin{table}[!ht]
  \begin{center}
   \setlength\extrarowheight{2pt} % for a bit of visual "breathing space"
    \rowcolors {2}{gray!15}{}
   \begin{tabular}{c c c c c c c}
    \toprule
    \textbf{Layer} & \textbf{Map} & \textbf{Tensor size} & \textbf{Kernel size} & \textbf{Stride} & \textbf{Activation} & \textbf{Params}\\
    \hline   
    Input Image     & 1     & 227x227x3 & -     & - & -     & - \\
    Convolution 1   & 96    &  55x55x96 & 11x11 & 4 & relu & 34,944 \\
    Max pooling     & 96    &  27x27x96 & 3x3   & 2 & relu & 0 \\
    Convolution 2   & 256   & 27x27x256 & 5x5   & 1 & relu & 614,656 \\
    Max pooling     & 256   & 13x13x256 & 3x3   & 2 & relu & 0 \\
    Convolution 3   & 384   & 13x13x384 & 3x3   & 1 & relu & 885,120 \\
    Convolution 4   & 384   & 13x13x384 & 3x3   & 1 & relu & 1,327,488 \\
    Convolution 5   & 256   & 13x13x256 & 3x3   & 1 & relu & 884,992 \\
    Max pooling     & 256   &  6x6x256  & 3x3   & 2 & relu & 0 \\
    Dense 1         & -     &  4096x1   & -     & - & relu & 37,752,832 \\
    Dense 2         & -     & 4096x1    & -     & - & relu & 16,781,312 \\
    Dense 3         & -     & 4096x1    & -     & - & relu & 4,097,000 \\
    Output Dense    & -     & 1000      & -     & - & Softmax & - \\
    \bottomrule
    \end{tabular}
    \end{center}
    \caption{AlexNet Architecture \cite{alexnet}}
    \label{table:alexnet}
\end{table}

\subsection{Residual networks}

We realize that most of the works of the state of the art  uses residual networks \cite{myronenko20183d} \cite{learnvolrepreCODE} or networks with skip-connections \cite{2020inpainting} \cite{superresolution} \cite{bermudez2018t1autoencoder}, so we will introduce deeper than other architectures.

The emergence of this network rises from deep plain neural networks (as Alexnet) problems on training.  Despite the general belief that the more layers these networks have the more it learns, an experiment made by He et. al. \cite{reslearning} shows that, in some cases, more layers are related to less accuracy even using regularization techniques like L2 or Dropout. Deep plain networks suffer from performance degradation due to the loss of detail in deeper layers and vanishing gradient problem

This residual learning approach is introduced by Kaiming He et. al. in 2016 \cite{reslearning} with the goal of reducing the complexity of deep neural networks. The main innovation of residual networks is that the layers "learn residual functions with reference to the layer inputs, instead of learning unreferenced functions" \cite{reslearning} as they said. This network obtains better performance than before with less complexity in training.

In plain networks, the layers are built to approximate a mapping function from the image to a target: $H(x)$. This is equivalent to approximate the residual of this function: $F(x) = H(x)-x$ and then get the original function as $F(x)+x$. This kind of layer is called \textbf{Residual Block} in which we estimate the residuals and then we add the original input to get our original target function. We can see the architecture of this block in figure \ref{fig:figs/ResidualBlock.png} , which is the picture of the original work. Thus, a \textbf{skip-connection} is include in the network. \textbf{This only consist in adding the input of a stack of layers to the output of this stack of layers}. We have to notice $F(x)$ and $x$ could not have same dimension, so we have to multiply by a linear projection the original input $x$ for matching dimensions. 

\imagen[0.5]{figs/ResidualBlock.png}{Residual Block Architecture \cite{reslearning}}


This paradigm of residual learning gives rise to other ResNet-based architectures, which differs in number of layers and building block compositions. This kind of networks uses residual blocks (there are also several residual block architecture) with skip-connections embedded in some architecture. Two main examples are the ResNet34 and Resnet50 networks, which use different kinds of residual blocks. In addition, He et. al. presents some differentes ResNet architectures which we show in figure \ref{fig:figs/resnet_archs.PNG}. 

In addition, an Autoencoder made with residual blocks made by A. Myronenko \cite{myronenko20183d} is shown in figure \ref{fig:figs/architecture_myronenko.PNG}, in which green blocks represent residual blocks with group normalization. Residual blocks are also used in Yu et. al. \cite{learnvolrepreCODE} work. 


\imagen[1]{figs/resnet_archs.PNG}{Resnet architectures presented in \cite{reslearning}}


\subsection{Other skip-connection-based architectures}

As we explained before, deep plain networks suffer from performance degradation due to the loss of detail in deeper layers and vanishing gradient problem. Residual networks address this problems through residual blocks. But there is a inner idea in ResNets which other architectures also use: \textbf{skip-connections}. In ResNet skip-connections are added in order to estimate the residual function, but these kind of skip-connections can be added to networks with another purpose. Other networks like some Fully Convolutional Networks also use this skip-connections. Other networks like some Fully Convolutional Networks also use this skip-connections. For example, FCN-8 architecture has skip connections, in which some feature maps of the earlier layers are added to later layers.

\textbf{Skip connections from one layer of the encoder to it symmetric layer of the autoencoder} are added to the architecture with 2 purposes. First, to allow the signal to backpropagate straight to the lower layers and thus address the problem of gradient disappearance, facilitating deep network training and, thus, achieving improvements in restoration performance. Second, when we build a deeper network, low-detail of the image could be lost, making deconvolution difficult in recovering task. However, the skip connections pass through the feature maps which carry much image detail and helps deconvolution to recover the original image. 

Some related works use this kind of architecture. C. Bermudez et. al \cite{bermudez2018t1autoencoder} uses skip symmetric connections as can be shown in figure \ref{fig:figs/cbermudezarchitecture.jpg}. This is a convolutional autoencoder with Leaky ReLU, in which the skip-connections add a layer in the encoder to their symmetric layer in the autoencoder (Like FCN but being symmetric).

However, some concrete architectures of CAE + skip-connections have been established due to their good results in some tasks. U-Net \cite{ronneberger2015unet} is one of them, and a U-Net-Autoencoder-based architecture is used by Manjon et. al. \cite{2020inpainting} which we can see in figure \ref{fig:figs/architecture_manjon.PNG}. 




\subsubsection{U-Net and V-Net}

\textbf{U-Net} \cite{ronneberger2015unet} is one of the networks that has skip-connections between symmetric layers. U-Net is a Fully-Convolutional-based Network (FCN) that is mainly used for image segmentation, that's why Manjón \cite{2020inpainting} use a V-Net based architecture for inpainting. The principal 2 differences between a FCN and U-net are the \textbf{symmetry} of U-NET and the skip connections between the downsampling (encoder) path and the upsampling (decoder) path which use \textbf{concatenation} operator instead of sum (sum operator is used in skip-connections in Fully Convolutional Networks).

Each of the 4 blocks used in the downsampling path is made up of 2 convolutional layers (3x3) with batch normalization and ReLU and another 2x2 MaxPooling layer. This block extract advanced features while reducing feature maps sizes. In the upsampling route, the 4 blocks used are made by a 2x2 upconvolutional layer and 2 other convolutional layers like those of the downsampling route to recover size of segmentation maps, the concatenation of the feature map of the symmetric layer of the encoder to give the location information from the encoding path to decoding path and a final 1x1 convolution. The original architecture of U-NET paper is illustrated in figure \ref{fig:figs/unet_architecture.png}. As we can see, this original U-Net architecture is very similar to Manjon et. al. U-Net based autoencoder; see figure \ref{fig:figs/architecture_manjon.PNG}.

\imagen[0.7]{figs/unet_architecture.png}{U-Net architecture of original paper: O Ronneberger et. al. \cite{ronneberger2015unet}}

U-Net has suffered some modifications, one of the most important is \textbf{V-Net}. This network was introduces by F. Miilletari et. al \cite{vnet}. V-Net is used for volumetric biomedical image segmentation and it gets a very good performance in this task. This network is used in the work or T. Estienne \cite{otherBraTS2020} to create a V-Net-based autoencoder to solve Brats 2018 challenge. V-Net combines the skip-symmetric-connections with residual blocks. It means that the encoder and decoder parts are built with residual blocks instead of convolutional blocks as U-Net. V-Net also changes the size of kernels and convolutions in respect to original U-Net, but the main change are the residual blocks. So, V-Net combine the 2 types of skip-connections we have spoken in this research of state of art: residual blocks and larger skip-connections between symetrich layers.


%%SECTION: SUMMARY
\section{Summary of related works}
\label{section:sumary_soa}

We have compiled some recent and prominent works in which they use reconstruction methods for different purposes. We have generally explained their architectures and approaches.

%% Arquitecturas
Although all the collected architectures are based on autoencoders, it has its differences in how the autoencoder is built. First of all, it differs in the main architecture of the blocks of the autoencoder. Different kind of networks like \textbf{ResNet, UNet, VNet, Simple CAE} or \textbf{AlexNet}. Also, in some of them use the Variational Autoencoder approach, even in \cite{myronenko20183d} combine ResNet and VAE. 
Furthermore, there are additional architecture characteristic in which studies differ. 

Regardless of the main architecture, one feature is shared by all related works (works that use Deep Learning for MRI reconstruction): \textbf{skip-connections}. In the works that use ResNet-based \cite{myronenko20183d} \cite{learnvolrepreCODE} architectures, skip-connections are implicit in residual blocks. In this architecture the skip-connection is used for represents the residual estimation function. In U-Net \cite{2020inpainting} and CAE with skip-connections architectures \cite{bermudez2018t1autoencoder} \cite{superresolution}, the skip-connection is used with 2 purposes: allow the signal to backpropagate straight to the lower layers and pass the image details from the convolutional layers to the deconvolutional layers, fighting the low-detail loss \cite{superresolution}. In conclusion with the architecture research, \textbf{the related work use skip connections architectures: ResNet-base and CAE+Skip-Connection-based (U-Net-based and more)}.

%% Contar diferentes loss y medidas: mse, psnr, correlacion, kl
%% Inpainted y super resolution usan MSE, alguno mas tb
The \textbf{loss function} used is also a critical issue. Most of the studies researched use pixel-wise $MSE$, which implicit improves the evaluation metrics $PSNR$ and $SSIM$. In addition, other metrics are also used. $KL divergence$ is added to loss function when VAE is used or $cross-entropy$ and $cross-covariance$ are added when semi-supervised autoencoder is used \cite{pinaya2019}. However, we will discuss in this project the benefits of using $PSNR$ or $SSIM$ directly in loss function. 

%Slices
We are going to work with \textbf{2D slices of 3D brain MRI volumes}. It means that  in our project we are going to reconstruct 2D brain MR images. In order to get 2D images from volumes, we have to get slices, as we can see in image \ref{fig:figs/xyz_slice.PNG}. We can get many 2D images from one brain volume. So, in the  preprocessing step, we firstly must choose \textbf{what brain MRI view we are going to work with}. For non-isotropic acquisitions, we should ideally slice them so that the slices are high resolution: select slices where voxel dimension remains equal for the 2 slice dimensions (i.e. 1x1x5 mm3 should transform into slices of 1x1 mm2 and not 1x5 mm2).

But main step in choosing 2D slices is not that, main step is choose slices which retrieves relevant information. A volume can be seen as a 3D head and some slices (i.e. from the sides) can not retrieve relevant information, because it will retrieve noise or bone parts, but it don't show information about brain structure. We can see this fact in the image \ref{fig:figs/xyz_slice_bad.PNG}, in which is shown the same volume of the image \ref{fig:figs/xyz_slice.PNG} but different slice. In order to get images with relevant information, we have recompiled some main methods in the state of art: \textbf{get fixed number of slices from all volumes, get the middle slice from the volume or develop ourselves a computer vision tool to evaluate the thickness (with opencv)  }. Although the first approach seems the simplest, it is the most used for its good results.

\imagen{figs/xyz_slice.PNG}{2D slices from brain volume IXI ID 002. Different profiles can be seen. Source: myself}

\imagen{figs/xyz_slice_bad.PNG}{2D slices from IXI ID 002. Slices from volume sides with no relevant information. Source: myself}


%% Contar diferentes preprocesados
\textbf{Brain MRI preprocessing} differs depending on the objective. First of all, some of the studies use the images of the dataset directly as the \textbf{target output} of the network. Other works \textbf{enhance the image quality and contrast} before sending it like target output. 
To continue, other preprocessing can be made when sending brain MRI to the input layers. \textbf{Downsampling} the input images could be also useful to reduce the number of parameters of the network. So the resolution of the input image should be balanced between usability (if it is very small is not useful at all) and trainability. We can \textbf{normalize the intensity} of pixels (values from 0 to 1 or mean 0 and standard deviation 1) is very important for a neural network.  Normalization of the inputs helps the training of the network due to several reasons. One of the most important is that a target variable with a large spread of values may result in large error gradient values causing weight values to change dramatically, making the learning process unstable \cite{bishop1995neural}. Besides, \textbf{affine transformations} like translation or rotation could be applied. In addition, we can realize \textbf{data augmentation} in real-time by adding Gaussian noise, other noise or cropping some parts (fixed rectangles or with lesion masks like lessionBrain \cite{lesionBrain}).
  


With this research of the art, we are ready to develop the next stages our approach to brain MRI reconstruction. We emphasize that the paper-discovery techniques used in this research are improved by new frameworks described in section \ref{section:papers_discovery}.

Finally, we made a recompilation of papers reviewed in Table 1 of D. Tamada, 2020 \cite{tamada2020review} and by our own, with some removals and additions based in our goal of the project (see \ref{table:paper_overview}). We have just deeply explained some of them. From the table of D. Tamada we only obtain 1 autoencoder study \cite{bermudez2018t1autoencoder}, 3 sCNN and DnCNN approaches \cite{kidoh2019scnnt1} \cite{ganHR3d} \cite{dncnnnoise2noise} and 1 GAN study.  The other papers have been compiled by our own (Autoencoder based: \cite{pinaya2019} \cite{myronenko20183d} \cite{gondara2016medicalautoencoder} \cite{superresolution} \cite{fuzzyautoencoder} \cite{learnvolrepreCODE} \cite{otherBraTS2020}, GAN-Autoencoder-based: \cite{wganautoencoder}).



\begin{table}[!ht]
    \setlength\extrarowheight{2pt} % for a bit of visual "breathing space"
    \rowcolors {2}{gray!15}{}
    \begin{tabularx}{\textwidth}{C C C}
    \toprule
        \textbf{Purpose} & \textbf{Year, Authors} & \textbf{Network} \\
        \hline
        
        \rowcolor{orange!10}
        \multicolumn{3}{c}{\textbf{Autoencoders}}\\
        
        \hline
        
        Identify brain abnormal structural patterns & 2018, W. Pinaya, et al \cite{pinaya2019} & \textbf{Semi-supervised autoencoder with SeLU and loss MSE+cross-variance} \\
        
        3D Tumor segmentation & 2018, A. Myronenko \cite{myronenko20183d} [Code available] & \textbf{VAE for regularization to encoder (ResNet like)} \\
        
         Lesion inpainting & 2020, J. V. Manjón et al \cite{2020inpainting} & \textbf{3D UNet autoencoder with skip-connections and upsampling at end} \\
         
         General image denoising and super resolution & 2016, XJ. Mao et al \cite{superresolution} [Code available] & \textbf{Residual CAE with symmetric skip connections} \\
        
        Learn Brain volumetric representation &  2018, Evan  M.  Yu  et.   al. \cite{learnvolrepreCODE} [Code available] & \textbf{STN+Residual CAE with skip connections, IN and LReLU}\\
        
        3D Tumor segmentation & 2020, T. Estienne \cite{otherBraTS2020} [Code available] & \textbf{VNET Autoencoder for regularization to encoder.} \\
        
        Denoising for T1 weighted brain MRI & 2018, C. Bermudez, et al \cite{bermudez2018t1autoencoder} & \textbf{Autoencoder with skip connections} \\
        
        Medical image denoise & 2016, L. Gondara, et al \cite{gondara2016medicalautoencoder} &\textbf{Convolutional denoising autoencoder} \\
        
        Brain MRI denoise & 2019, N. Chauhan et al \cite{fuzzyautoencoder} & \textbf{Convolutional denoising autoencoder with Fuzzy Logic filters} \\

        \hline
        \rowcolor{orange!10}
        \multicolumn{3}{c}{\textbf{sCNN and DnCNN}}\\
        \hline

        Denoising for T1, T2 and FLAIR brain images & 2018, M. Kidoh, et al \cite{kidoh2019scnnt1} & Single-scale CNN with DCT \\
        
        Motion artifact reduction for brain MRI & 2018, P. Johnson, et al \cite{scnnmotion} & Single-scale CNN\\
        
        Denoising for multishot DWI & 2020, M Kawamura et al \cite{dncnnnoise2noise} & DnCNN with Noise2Noise \\
        
        \hline
        \rowcolor{orange!10}
        \multicolumn{3}{c}{\textbf{GAN}}\\
        \hline
        
        Motion artifact reduction for brain MRI & 2018 BA. Duffy, et al \cite{ganHR3d} & GAN with HighRes3dnet as generator \\
        
        Denoise 3D MRI & 2019, M. Ran et al \cite{wganautoencoder} & \textbf{Wasserstein GAN with Convolutional Autoencoder generator} \\
 
    \bottomrule
    \end{tabularx}
    \caption{Overview of studies for reconstruction based in Table 1 from D. Tamada \cite{tamada2020review} (In bold the autoencoder related architecture)}
    \label{table:paper_overview}
\end{table}
\FloatBarrier


\section{New paper-discovery frameworks}
\label{section:papers_discovery}

In this state of art research, we have use new techniques and frameworks among the classical ones. \myurl{https://www.connectedpapers.com/}{Connected Papers} is a network science framework to improve the search of papers. There are also other platforms like \myurl{https://paperswithcode.com/}{Papers With Code} and \myurl{https://distill.pub/}{Distill} that improve the experience of article discovering and article visualization-interaction.
 
I will explain some little examples of paper discovery using this tools. \textit{Connected Papers} retrieve us a graph about the relationship of a given paper. A paper is related with another if one is cited by the other. In addition, the graph contains papers citated by the succesors of the main one. So, the graph contains the most important prior works and Derivate works of our main paper, giving us a perfect tool for paper discovery. \myurl{https://www.connectedpapers.com/main/37a18be8c599b781cc28b6a62d8f11e8a6a75169/3D-MRI-brain-tumor-segmentation-using-autoencoder-regularization/graph}{One example} made for A. Myronenko work \cite{myronenko20183d} is shown in figure \ref{fig:figs/connected_papers_myronenko.PNG} 
 
 \imagen{figs/connected_papers_myronenko.PNG}{Graph of connected papers for A. Myronenko 2018 \cite{myronenko20183d}}

\textit{Papers with code} is a web page, in which are stored papers with it official code implementations. It also group works in different subjects of study, like Medical issues, image segmentation, etc. In addition, it recompile the benchmarks for different machine learning tasks, and it make a ranking of the papers (with the code an pdf linked from the page). We show in figure \ref{fig:figs/pwc_myronenko.png} an example of the Myronenko work. We can see the abstract, the link of the paper and multiple links for implementations. Also, we can see the tasks and the benchmark results. There are more thing that we can do and research in this framework. 

\imagen{figs/pwc_myronenko.png}{A. Myronenko work \cite{myronenko20183d} in Papers with code}